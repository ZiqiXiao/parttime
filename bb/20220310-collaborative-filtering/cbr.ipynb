{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [100836, 80669]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [74]\u001B[0m, in \u001B[0;36m<cell line: 25>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# 训练KNN回归模型并进行交叉验证\u001B[39;00m\n\u001B[1;32m     24\u001B[0m knn \u001B[38;5;241m=\u001B[39m KNeighborsRegressor(n_neighbors\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m)\n\u001B[0;32m---> 25\u001B[0m scores \u001B[38;5;241m=\u001B[39m \u001B[43mcross_val_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43mknn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrating\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# 输出平均交叉验证精度\u001B[39;00m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMean CV score:\u001B[39m\u001B[38;5;124m'\u001B[39m, scores\u001B[38;5;241m.\u001B[39mmean())\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:515\u001B[0m, in \u001B[0;36mcross_val_score\u001B[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;66;03m# To ensure multimetric format is not supported\u001B[39;00m\n\u001B[1;32m    513\u001B[0m scorer \u001B[38;5;241m=\u001B[39m check_scoring(estimator, scoring\u001B[38;5;241m=\u001B[39mscoring)\n\u001B[0;32m--> 515\u001B[0m cv_results \u001B[38;5;241m=\u001B[39m \u001B[43mcross_validate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    516\u001B[0m \u001B[43m    \u001B[49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    517\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    518\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    519\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgroups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    520\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscoring\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mscore\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mscorer\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    521\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    522\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfit_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    525\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpre_dispatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpre_dispatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    526\u001B[0m \u001B[43m    \u001B[49m\u001B[43merror_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    527\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    528\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cv_results[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_score\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:252\u001B[0m, in \u001B[0;36mcross_validate\u001B[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcross_validate\u001B[39m(\n\u001B[1;32m     50\u001B[0m     estimator,\n\u001B[1;32m     51\u001B[0m     X,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     63\u001B[0m     error_score\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mnan,\n\u001B[1;32m     64\u001B[0m ):\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;124;03m\"\"\"Evaluate metric(s) by cross-validation and also record fit/score times.\u001B[39;00m\n\u001B[1;32m     66\u001B[0m \n\u001B[1;32m     67\u001B[0m \u001B[38;5;124;03m    Read more in the :ref:`User Guide <multimetric_cross_validation>`.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;124;03m    [0.28009951 0.3908844  0.22784907]\u001B[39;00m\n\u001B[1;32m    251\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 252\u001B[0m     X, y, groups \u001B[38;5;241m=\u001B[39m \u001B[43mindexable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    254\u001B[0m     cv \u001B[38;5;241m=\u001B[39m check_cv(cv, y, classifier\u001B[38;5;241m=\u001B[39mis_classifier(estimator))\n\u001B[1;32m    256\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m callable(scoring):\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:433\u001B[0m, in \u001B[0;36mindexable\u001B[0;34m(*iterables)\u001B[0m\n\u001B[1;32m    414\u001B[0m \u001B[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001B[39;00m\n\u001B[1;32m    415\u001B[0m \n\u001B[1;32m    416\u001B[0m \u001B[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    429\u001B[0m \u001B[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001B[39;00m\n\u001B[1;32m    430\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    432\u001B[0m result \u001B[38;5;241m=\u001B[39m [_make_indexable(X) \u001B[38;5;28;01mfor\u001B[39;00m X \u001B[38;5;129;01min\u001B[39;00m iterables]\n\u001B[0;32m--> 433\u001B[0m \u001B[43mcheck_consistent_length\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    434\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:387\u001B[0m, in \u001B[0;36mcheck_consistent_length\u001B[0;34m(*arrays)\u001B[0m\n\u001B[1;32m    385\u001B[0m uniques \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(lengths)\n\u001B[1;32m    386\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 387\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    388\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound input variables with inconsistent numbers of samples: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    389\u001B[0m         \u001B[38;5;241m%\u001B[39m [\u001B[38;5;28mint\u001B[39m(l) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m lengths]\n\u001B[1;32m    390\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: Found input variables with inconsistent numbers of samples: [100836, 80669]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 读取电影数据\n",
    "movies = pd.read_csv('ml-latest-small/movies.csv')\n",
    "\n",
    "# 读取评级数据\n",
    "ratings = pd.read_csv('ml-latest-small/ratings.csv')\n",
    "\n",
    "# 创建一个新的数据框，将电影和评级数据合并\n",
    "data = pd.merge(movies, ratings, on='movieId')\n",
    "\n",
    "# 使用TF-IDF向量化电影的特征\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(data['genres'])\n",
    "\n",
    "# 将评级数据分成训练集和测试集\n",
    "train = data.sample(frac=0.8, random_state=1)\n",
    "test = data.drop(train.index)\n",
    "\n",
    "# 训练KNN回归模型并进行交叉验证\n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "scores = cross_val_score(knn, X, train['rating'], cv=5)\n",
    "\n",
    "# 输出平均交叉验证精度\n",
    "print('Mean CV score:', scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CV score: -0.29424166869990065\n",
      "RMSE: 1.1510068548396815\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 读取电影数据\n",
    "movies = pd.read_csv('ml-latest-small/movies.csv')\n",
    "\n",
    "# 读取评级数据\n",
    "ratings = pd.read_csv('ml-latest-small/ratings.csv')\n",
    "\n",
    "# 将评级数据分成训练集和测试集\n",
    "train_ratings = ratings.sample(frac=0.8, random_state=1)\n",
    "test_ratings = ratings.drop(train_ratings.index)\n",
    "\n",
    "# 创建一个新的数据框，将电影和评级数据合并\n",
    "train_data = pd.merge(movies, train_ratings, on='movieId')\n",
    "test_data = pd.merge(movies, test_ratings, on='movieId')\n",
    "\n",
    "# 使用TF-IDF向量化电影的特征\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train = vectorizer.fit_transform(train_data['genres'])\n",
    "X_test = vectorizer.transform(test_data['genres'])\n",
    "\n",
    "# 训练KNN回归模型并进行交叉验证\n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "knn.fit(X_train, train_data['rating'])\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# 输出平均交叉验证精度和均方根误差\n",
    "cv_scores = cross_val_score(knn, X_train, train_data['rating'], cv=5)\n",
    "print('Mean CV score:', cv_scores.mean())\n",
    "print('RMSE:', mean_squared_error(test_data['rating'], y_pred, squared=False))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 5\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "TOP 10\n",
      "Precision: 0.00016420361247947455\n",
      "Recall: 0.00016420361247947455\n",
      "TOP 15\n",
      "Precision: 0.0004378762999452655\n",
      "Recall: 0.0004378762999452655\n",
      "TOP 20\n",
      "Precision: 0.0006568144499178982\n",
      "Recall: 0.0006568144499178982\n",
      "TOP 25\n",
      "Precision: 0.001050903119868637\n",
      "Recall: 0.001050903119868637\n",
      "TOP 30\n",
      "Precision: 0.0010399562123700053\n",
      "Recall: 0.0010548127296895768\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# 读取电影数据\n",
    "movies = pd.read_csv('ml-latest-small/movies.csv')\n",
    "\n",
    "# 读取评级数据\n",
    "ratings = pd.read_csv('ml-latest-small/ratings.csv')\n",
    "\n",
    "# 使用电影ID将电影和评级数据合并\n",
    "data = pd.merge(movies, ratings, on='movieId')\n",
    "\n",
    "# 使用TF-IDF向量化电影的特征\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(data['genres'])\n",
    "\n",
    "# 训练KNN模型\n",
    "knn = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "knn.fit(X)\n",
    "\n",
    "# 获取每个电影的邻居\n",
    "neighbors = knn.kneighbors(X, n_neighbors=30, return_distance=False)\n",
    "\n",
    "# 将邻居矩阵转换为DataFrame\n",
    "neighbors_df = pd.DataFrame(neighbors, index=data.index)\n",
    "\n",
    "# 将评级数据按用户ID分成训练集和测试集\n",
    "train = ratings.sample(frac=0.8, random_state=1)\n",
    "test = ratings.drop(train.index)\n",
    "\n",
    "# 在测试集上计算精度和召回率\n",
    "for n in [5, 10, 15, 20, 25, 30]:\n",
    "    precision_sum = 0\n",
    "    recall_sum = 0\n",
    "    count = 0\n",
    "    for user_id in test['userId'].unique():\n",
    "        user_ratings = test[test['userId'] == user_id]\n",
    "        if len(user_ratings) == 0:\n",
    "            continue\n",
    "        user_movies = user_ratings.sort_values('rating', ascending=False)['movieId'][:n]\n",
    "        relevant_movies = set(user_movies)\n",
    "        recommended_movies = set(neighbors_df.loc[user_ratings.index, :n].values.reshape(-1))\n",
    "        common_movies = relevant_movies.intersection(recommended_movies)\n",
    "        precision = len(common_movies) / n\n",
    "        recall = len(common_movies) / len(relevant_movies)\n",
    "        precision_sum += precision\n",
    "        recall_sum += recall\n",
    "        count += 1\n",
    "    precision = precision_sum / count\n",
    "    recall = recall_sum / count\n",
    "    print('TOP', n)\n",
    "    print('Precision:', precision)\n",
    "    print('Recall:', recall)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 943/943 [00:34<00:00, 27.05it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [83]\u001B[0m, in \u001B[0;36m<cell line: 50>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;66;03m# 计算precision和recall\u001B[39;00m\n\u001B[1;32m     49\u001B[0m topn \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n\u001B[0;32m---> 50\u001B[0m precision, recall \u001B[38;5;241m=\u001B[39m \u001B[43mprecision_recall\u001B[49m\u001B[43m(\u001B[49m\u001B[43muser_recs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtopn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtopn = \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m时，precision = \u001B[39m\u001B[38;5;132;01m%.3f\u001B[39;00m\u001B[38;5;124m, recall = \u001B[39m\u001B[38;5;132;01m%.3f\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m (topn, precision, recall))\n",
      "Input \u001B[0;32mIn [83]\u001B[0m, in \u001B[0;36mprecision_recall\u001B[0;34m(user_recs, test_df, topn)\u001B[0m\n\u001B[1;32m     39\u001B[0m rec_items \u001B[38;5;241m=\u001B[39m [i[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28msorted\u001B[39m(user_recs[user_id], key\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m x: x[\u001B[38;5;241m1\u001B[39m], reverse\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)[:topn]]\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# 计算precision\u001B[39;00m\n\u001B[0;32m---> 41\u001B[0m user_precision\u001B[38;5;241m.\u001B[39mappend(\u001B[43mprecision_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_items\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrec_items\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m# 计算recall\u001B[39;00m\n\u001B[1;32m     43\u001B[0m user_recall\u001B[38;5;241m.\u001B[39mappend(recall_score(test_items, rec_items))\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1769\u001B[0m, in \u001B[0;36mprecision_score\u001B[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001B[0m\n\u001B[1;32m   1640\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprecision_score\u001B[39m(\n\u001B[1;32m   1641\u001B[0m     y_true,\n\u001B[1;32m   1642\u001B[0m     y_pred,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1648\u001B[0m     zero_division\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwarn\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1649\u001B[0m ):\n\u001B[1;32m   1650\u001B[0m     \u001B[38;5;124;03m\"\"\"Compute the precision.\u001B[39;00m\n\u001B[1;32m   1651\u001B[0m \n\u001B[1;32m   1652\u001B[0m \u001B[38;5;124;03m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1767\u001B[0m \u001B[38;5;124;03m    array([0.5, 1. , 1. ])\u001B[39;00m\n\u001B[1;32m   1768\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1769\u001B[0m     p, _, _, _ \u001B[38;5;241m=\u001B[39m \u001B[43mprecision_recall_fscore_support\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1770\u001B[0m \u001B[43m        \u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1771\u001B[0m \u001B[43m        \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1772\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1773\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpos_label\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpos_label\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1774\u001B[0m \u001B[43m        \u001B[49m\u001B[43maverage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maverage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1775\u001B[0m \u001B[43m        \u001B[49m\u001B[43mwarn_for\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprecision\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1776\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1777\u001B[0m \u001B[43m        \u001B[49m\u001B[43mzero_division\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mzero_division\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1778\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1779\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m p\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1556\u001B[0m, in \u001B[0;36mprecision_recall_fscore_support\u001B[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001B[0m\n\u001B[1;32m   1554\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m beta \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1555\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbeta should be >=0 in the F-beta score\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1556\u001B[0m labels \u001B[38;5;241m=\u001B[39m \u001B[43m_check_set_wise_labels\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maverage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpos_label\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m samplewise \u001B[38;5;241m=\u001B[39m average \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msamples\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1374\u001B[0m, in \u001B[0;36m_check_set_wise_labels\u001B[0;34m(y_true, y_pred, average, labels, pos_label)\u001B[0m\n\u001B[1;32m   1372\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m y_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmulticlass\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1373\u001B[0m             average_options\u001B[38;5;241m.\u001B[39mremove(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msamples\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1374\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1375\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTarget is \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m but average=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbinary\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. Please \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1376\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mchoose another average setting, one of \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (y_type, average_options)\n\u001B[1;32m   1377\u001B[0m         )\n\u001B[1;32m   1378\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m pos_label \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m   1379\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m   1380\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNote that pos_label (set to \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m) is ignored when \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1381\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maverage != \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbinary\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m (got \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m). You may use \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1384\u001B[0m         \u001B[38;5;167;01mUserWarning\u001B[39;00m,\n\u001B[1;32m   1385\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 加载数据\n",
    "df = pd.read_csv('ml-100k/u.data', sep='\\t', names=['user_id', 'item_id', 'rating', 'timestamp'],\n",
    "                 encoding='latin-1')\n",
    "# 计算每部电影的平均评分\n",
    "item_mean_rating = df.groupby('item_id')['rating'].mean()\n",
    "# 根据电影内容特征，计算电影之间的相似度\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "item_features = pd.read_csv('ml-100k/u.item', sep='|',\n",
    "                            encoding='latin-1', names=['item_id', 'title', 'release_date', 'video_release_date',\n",
    "                                                              'imdb_url', 'unknown', 'action', 'adventure', 'animation',\n",
    "                                                              'childrens', 'comedy', 'crime', 'documentary', 'drama',\n",
    "                                                              'fantasy', 'film_noir', 'horror', 'musical', 'mystery',\n",
    "                                                              'romance', 'sci_fi', 'thriller', 'war', 'western'])\n",
    "item_features = item_features.drop(columns=['title', 'release_date', 'video_release_date', 'imdb_url', 'unknown'])\n",
    "item_similarity = cosine_similarity(item_features.values)\n",
    "# 计算每个用户的推荐列表\n",
    "from tqdm import tqdm\n",
    "user_recs = {}\n",
    "for user_id in tqdm(df['user_id'].unique()):\n",
    "    user_items = df[df['user_id'] == user_id]['item_id'].values\n",
    "    user_sim_items = []\n",
    "    for item_id in user_items:\n",
    "        sim_scores = list(enumerate(item_similarity[item_id-1]))\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "        sim_scores = sim_scores[1:11]\n",
    "        user_sim_items += [i[0] for i in sim_scores]\n",
    "    user_sim_items = list(set(user_sim_items))\n",
    "    user_recs[user_id] = [(item, item_mean_rating[item]) for item in user_sim_items]\n",
    "# 推荐TOPN的precision和recall\n",
    "from sklearn.metrics import precision_score, recall_score\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topn = 10时，precision = 0.002, recall = 0.002\n"
     ]
    }
   ],
   "source": [
    "def precision_recall(user_recs, test_df, topn):\n",
    "    user_precision = []\n",
    "    user_recall = []\n",
    "    for user_id in user_recs.keys():\n",
    "        # 取出该用户的测试集\n",
    "        test_items = test_df[test_df['user_id'] == user_id]['item_id'].values\n",
    "        # 取出topn的推荐列表\n",
    "        rec_items = [i[0] for i in sorted(user_recs[user_id], key=lambda x: x[1], reverse=True)[:topn]]\n",
    "        # 计算precision\n",
    "        user_precision.append(precision_score(test_items, rec_items, average='micro'))\n",
    "        # 计算recall\n",
    "        user_recall.append(recall_score(test_items, rec_items, average='micro'))\n",
    "    return (sum(user_precision) / len(user_precision), sum(user_recall) / len(user_recall))\n",
    "\n",
    "# 加载测试集\n",
    "test_df = pd.read_csv('ml-100k/ua.test', sep='\\t', names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "# 计算precision和recall\n",
    "topn = 10\n",
    "precision, recall = precision_recall(user_recs, test_df, topn)\n",
    "print('topn = %d时，precision = %.3f, recall = %.3f' % (topn, precision, recall))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
